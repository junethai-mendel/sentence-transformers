model:
  name: "nvidia/NV-Embed-v2"
  max_seq_length: 1024
  tokenizer_padding_side: "right"
  set_pooling_include_prompt: false

peft:
  embedding_model:
    target_modules: ["embed_tokens", "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  latent_attention_model:
    target_modules: ["to_q", "to_kv", "to_out", "net.0", "net.2"]
  inference_mode: false
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1

data:
  hf_data_dir: "/workspace/data/june/sentence-transformers/data/hf_v5"

training:
  output_dir: "models/nv-embed-v2-hfv5-peft"
  num_train_epochs: 1
  train_batch_size: 4
  eval_batch_size: 64
  gradient_accumulation_steps: 1
  learning_rate: 1e-5
  warmup_ratio: 0.2
  fp16: false
  bf16: true
  eval_strategy: "steps"
  eval_steps: 1000
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 2
  logging_steps: 100
  logging_first_step: true
  seed: 6789
  run_name: "nv-embed-v2-hfv5-peft"

evaluation:
  name: "hfv5-test"

output:
  model_save_path: "models/nv-embed-v2-hfv5-peft/final"
  run_name: "MendelAI/nv-embed-v2-hfv5-peft"
  private: true
